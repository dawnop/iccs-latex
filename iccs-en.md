# Analysis of the Design of Several Modern Programming Languages

## Abstract

Modern programming languages are analyzed in terms of design through several aspects of programming language theory - programming paradigm, type system, and performance.
The programming paradigm section gives a method for evaluating the degree of programming paradigm support and uses this method to evaluate the degree of programming paradigm support in modern programming languages. Thus, the connection between programming paradigms and application scenarios is obtained.
The type system section analyzes the commonalities of the type systems of modern programming languages.
The performance section quantitatively analyzes the performance of modern programming languages by means of benchmarking, and qualitatively analyzes the connection between the performance of programming languages and application scenarios.
Finally, it is concluded that the design and practice of programming languages evolve dynamically, and the design of programming languages is closely related to their application scenarios.

## Introduction

In programming language theory(PLT), programming language design has always been an important topic. Because programming language design integrates various branches of PLT, it is the prerequisite for the final realization of programming language. By analyzing the design of modern programming languages(MPL), we can obtain the trend of programming language development, i.e., the programming language design can be fed back from the application point of view.

By and large, the overall amount of literature on the analysis of programming language design is relatively small. M Coblenz argues that programming language design should be considered in terms of several theories related to programming languages and gives a way of evaluating programming language design. A Stefika gives some issues to consider in programming language design and argues that type systems are crucial for programming languages. However, the above results focus on a qualitative analysis of programming language design and lack some concrete examples. LA Meyerovich analyzes the usage of popular programming languages to give best practices in programming language design through statistics. This analytical approach is somewhat lacking in theoretical and systematic aspects. F Morandat provides a systematic analysis of the R language, including performance, syntactic design, and application scenarios. The article adopts a better research approach and can be used to broaden the analysis perspective based on that article. Currently, there is a lack of a systematic, theoretical and practical, wide-ranging, application-oriented analysis of programming language design.

By analyzing the design of MPL, this paper draws the influence of different programming language factors on the application. e.g., as programming languages evolve, why does the level of paradigm support in MPLs keep changing, what type systems are in MPLs, how MPLs keep programs efficient, and so on.

## Several modern programming languages

There is no uniform definition for MPL. Many researchers define it in different ways, such as finding connections between software engineering and the development of MPLs or elaborating around abstractions. The above are correct in their respective fields, and here the definition is abstracted and extended to adapt to more domains. By studying the intersection of different definitions, a more general definition of MPLs derives. It is an obvious fact, that as the practice of programming languages continues to deepen, so does the definition of MPL. This is because PLT is a field where practice and theory go hand in hand. Although its definition is constantly changing, some core elements remain the same. Here are a few features of MPLs.

1. Substance over form. It should provide descriptive grammatical structures, rather than hand-in-hand telling the machine what to do. This item emphasizes the abstraction of machine functions.
2. Semantic Consistency. For similar grammatical structures, there should be similar grammatical functions.
3. Syntax bootstrapping. For non-core grammatical features, based on following semantic consistency, they should be composed of core grammatical features.
4. Paradigm convergence. Multiple programming paradigms should be provided without forcing users to use a particular programming paradigm.

### Select modern programming languages to be analyzed

For the selection of programming language, the first thing is to pay attention to the most popular trend at the moment, so the selected programming language must cover the most commonly used. Secondly, there are many classification standards for programming languages. For each division standard, the selected programming language should cover most of the options. Then, it should focus on highlighting the new programming language with excellent design in recent years. These programming languages have absorbed the advantages of past programming languages and improved their inherent shortcomings. From these selected MPLs, we can see the development trend of application-oriented programming languages over the years.

According to the data of the *IEEE spectrum*, the top five most popular programming languages in 2021 are Python, Java, C, C ++, and JavaScript. An exception case is C, which provides low-level access to computer systems. To be precise, the syntax design of the C language does not conform to any of the features of MPL. And for the four remaining popular programming languages mentioned above, they are still helpful even though they are not exactly in line with the features of MPL. In the era when these languages were first created, they emerged as representatives of MPLs. Therefore these four past programming languages are compared with the emerging MPLs.

Based on the above-mentioned MPL features, several programming languages were selected and arranged in order of popularity, resulting in Go, Swift, Dart, Rust and Kotlin (where Swift and Dart are equally popular). Coincidentally, these languages are also released in this order sequentially. In addition, all of these languages have similar type systems and all support AOT compilation, and although not all use garbage collection memory management, they all move away from manual memory management. These are all common features of MPLs in today's application environment.

| Language   | Programming Paradigm | Type System           | Compilation Mode | Memory Model | Release Date | Application Scenarios        |
| ---------- | -------------------- | --------------------- | ---------------- | ------------ | ------------ | ---------------------------- |
| Python     | Multi-paradigm       | Dynamically, Strongly | JIT              | GC           | 1991         | Web, Enterprise, Embedded    |
| Java       | Multi-paradigm       | Statically, Strongly  | AOT              | GC           | 1995         | Web, Mobile, Enterprise      |
| C++        | Multi-paradigm       | Statically, Weakly    | AOT              | Manual       | 1983         | Mobile, Enterprise, Embedded |
| JavaScript | Multi-paradigm       | Untyped               | JIT              | GC           | 1995         | Web                          |
| Go         | Multi-paradigm       | Statically, Strongly  | AOT              | GC           | 2009         | Web, Enterprise              |
| Swift      | Multi-paradigm       | Statically, Strongly  | AOT              | ARC          | 2014         | Mobile, Enterprise           |
| Dart       | Multi-paradigm       | Statically, Strongly  | AOT&JIT          | GC           | 2011         | Web, Mobile                  |
| Rust       | Multi-paradigm       | Statically, Strongly  | AOT              | Ownership    | 2015         | Web, Enterprise, Embedded    |
| Kotlin     | Multi-paradigm       | Statically, Strongly  | AOT&JIT          | GC           | 2016         | Web, Mobile                  |

### How to evaluate the design of programming languages

The evaluation criteria here are more application-oriented, to evaluate whether a programming language is "practical" or not. Rather than evaluating whether a programming language is "elegant" as in the features of MPL. In fact, most of the popular languages are not elegant. This is because the development of programming languages is not just a technical matter, but involves political, commercial, and other external factors. A typical example is the evolution of JavaScript. Today, the language standards for JavaScript in mainstream browsers are still not uniform. But this does not prevent JavaScript from being the most popular language for Web applications. But that doesn't stop JavaScript from being the most useful language for Web applications. There are two main aspects that determine whether a language is useful or not. One aspect is its syntactic design. For complex business logic, programming languages are required to provide strong expressiveness, i.e., isolate underlying implementations that are not related to the business logic to accommodate rapid changes. Programming languages are also needed to provide solutions for checking the correctness and reliability of programs. Another aspect is performance, where programming languages are always expected to have a low memory overhead and time overhead, relying heavily on compile-time (native languages) and run-time (virtual machine languages) optimizations.

In practice, however, programming language design cannot be accurately quantified. The reason for this is twofold. The first is that certain criteria for evaluation are not quantitative. For expressiveness and reliability, both are abstract descriptions, and they have uncountable kinds of cases in practice. If one wants to analyze them quantitatively, then one must restrict the analysis to certain specific application scenarios. The second is that the evaluation criteria are not sufficient. Some evaluation criteria that are difficult to measure, such as response time in a real-time system, are dropped here for structural completeness. For programming languages with garbage collectors, the performance loss from garbage collection is not negligible. In particular, JVM-based programming languages have the problem of "Stop the World" when garbage collection is performed, which affects the performance of the programming language. However, this is not considered for the sake of simplicity.

## Analysis of programming paradigms

A programming paradigm is the lowest level of design for a programming language. Theoretically, programming languages need to help people model the real world. Therefore, it is necessary to consider what modeling infrastructure should be provided, i.e., the programming paradigm of the programming language. In practice, programming languages should be designed with first-class citizens in mind, as the basic unit of the programming language. 

There is a certain conflict between different programming paradigms, stemming from the conflict of first-class citizens. In particular, it is very difficult to add a new paradigm to an already defined paradigm programming language. A typical example is Java. The functional features added by Java8 do not fundamentally change the Java programming paradigm, but are at best syntactic sugar. Due to the lack of the original programming paradigm design, Java is hardly fully compatible with functional features. Therefore functional programming in Java are crippled and inelegant. Therefore, for programming languages, especially for MPLs with multiple paradigms, the programming paradigm, and thus the first-class citizens, should be determined first.

Functional Programming (FP) and Object Oriented Programming (OOP) are the two most common, and at the same time the most important paradigms in MPLs. Therefore, these two paradigms are chosen for analysis when discussing programming paradigms below.

### Language, paradigm and concept

 The figure shows the relationship between languages, paradigms, and concepts. Each paradigm contains a core set of concepts, and each language also realizes one or more paradigms. FP and OOP are programming paradigms that are commonly available in MPLs. However, the degree of support for them varies from one programming language to another. The table shows the degree of support for the concepts of FP and OOP in each language.

For the concepts in the programming paradigm, a more practice-oriented organization is here. It will favor the choice of concepts defined in specific programming languages over those in programming paradigm theory. For example, for the functional concept of nested and anonymous functions, they are implemented as lambda expressions in many programming languages. Again, for example, for the object-oriented concept of combination, most of the currently popular programming languages support it. For programming language design, the combination is simply a way of arranging data. It does not belong to any paradigm. Even languages like the more ancient one, C, provide structure combinations and function combinations. Therefore, combinations are not included in the analysis.

### Functional programming

The most important feature of FP is its high degree of abstraction. This is mainly because FP has its origins in the lambda calculus, so it has some of the characteristics of abstract algebra. In the practice of FP, the main idea is the combination and nesting of higher-order functions, so the core of its design is to sort out the rules of nesting and combination of higher-order functions. Thus, the basic design approach can be described as follows. Based on the standardization of the function transfer model and the combinability of higher-order functions, the description of the mapping of data from the source to the result is completed through a series of rule designs. Here the mapping is done by a formal combination of several higher-order functions. Descriptions are like mathematical formulas that map input data into results through layers of iterations. For the iterative process of data in a broad sense, which includes not only the iteration of the data itself, but also the iteration of the rules. This is due to the fact that in functional languages, data and rules are semantically unified in functions.

As programming language practice continues to deepen, programming languages are becoming more and more supportive of functional concepts. For programming languages released in the 1980s and 1990s, the support for functional concepts was not good when they were first created. This is especially true for Java and C++. From the original syntax, one did not design them with FP in mind. And for JavaScript and Python, they are a bit better in this regard. Although they inherently support some functional concepts, they are still missing some advanced functional concepts. As for the programming languages released around 2010, they all support the basic functional concept. In particular, Rust and Kotlin additionally support the concept of "statements as expressions", a milestone in the development of MPL paradigm convergence. The concept of "statements as expressions" is more common in functional languages and provides semantic unification of expressions and statements. However, this concept is not supported in most programming languages with paradigm convergence. As programming languages continue to evolve, Kotlin and Rust are supporting this concept with paradigm convergence. Thus Kotlin and Rust can be considered as having a higher level of support for FP concepts.

### Object oriented programming

The most important feature of OOP is its emphasis on reuse. Back in the 1960s, software maintenance became increasingly difficult due to the increasing complexity of hardware and software. OOP solves this problem by emphasizing reusability. In the practice of OOP, one maps real problems into entities and their relationships, rather than being concerned with the process of dealing with the problem. After the birth of object-oriented languages, there was an urgent need to map from problems to entities and relationships in a modeling way, at which point UML was born. It is a set of standardized modeling languages for visualization and has greatly contributed to the development of object-oriented methodologies. Since then the practice of OOP has always emphasized design before implementation.

For the core concepts of OOP, there is not much difference in the level of support of MPLs. Because of its relatively long history and the lack of revolutionary innovations in theory and practice in recent years, most programming languages have implemented the core concepts of OOP relatively completely. In addition, more and more programming languages are realizing object-oriented features by supporting combinations and delegates, while mere inheritance has proven to be bad practice. Therefore, the concept of classes and inheritance is abandoned in Go and Rust, and object orientation is realized through trait. Compared to class-based object-oriented, trait-based object-oriented has a looser coupling and more flexible realization. However, most MPLs support both trait and class for different granularity of control.

In order to more accurately distinguish the degree of OOP concept support for each MPL, some concepts that are not commonly used are introduced here. An example is "everything is object". According to the definition of OOP, it should have been the most basic concept in OOP. In fact, however, early programming languages tended to have a large number of imperative features, i.e., not all elements were treated as objects. For example, there are still primitive data types in Java, which are not objects, so we cannot call methods of these types as if they were objects. Perhaps there are many performance positives of primitive data types, but from a semantic consistency perspective, primitive data types have negative effects. Therefore, languages with the concept of "everything is object" are considered to have a higher level of OOP support.

### Programming Paradigms and Applications

The above discussion leads to this diagram, which roughly depicts the degree of support for FP and OOP concepts in different programming languages. For a given programming language, the closer it is to FP/OOP in the figure, the better it supports FP/OOP. In fact, the paradigm of a programming language is very closely related to its application scenario, whether it is for a single application area or for multiple application areas. 

A typical example is Dart. Its main application scenario is the Web front-end, and it is often used as a support language for the GUI framework Flutter. It is obvious that most of the GUI frameworks we use have a complex inheritance structure. This is because, for GUIs, most of its application scenarios satisfy the Liskov Substitution Principle, i.e., the child type can completely replace the parent type. This is a sufficient condition for using inheritance. Therefore, Dart only provides object-oriented concepts based on inheritance.

The next example is Java, which has weak support for both FP and OOP concepts. In the early years, Java was used for Enterprise development. Later it was used for Web servers, a scenario that required high abstraction of business logic, so Java added the additional concept of FP. However, due to the design of the language itself, the level of support is not high.

Another example is Kotlin, which has better support for both FP and OOP. Its initial application scenario is Android. In order to solve the previous problem that Java was too cumbersome to develop Android applications, Kotlin was designed to add a lot of FP and OOP concepts for the Android application scenario. Not only does it support traditional FP and OOP concepts, but it also makes syntax-level optimizations for these concepts, such as the FP syntactic sugar "trailing lambda" and the OOP delegate syntactic sugar "by". These useful features in turn allow Kotlin to be used in other scenarios, such as server front-ends and back-ends.

## Analysis of type systems

The discussion here is based on L Cardelli's definition of type systems. This definition considers that the fundamental purpose of the type system is to prevent errors that occur during the runtime of a program, and therefore defines the type system by defining concepts related to errors. Good behavior is first defined by defining what is an error. Then it is divided into dynamic checking and static checking based on the timing of the programming language's behavior checking. On the basis of static checking, strong type checking and weak type checking are classified by whether the programming language can check forbidden errors. For the types of variables in programming languages, typed (static) languages and untyped (static) languages are classified according to whether or not there are static types to limit the range of variable types at runtime. Then explicitly typed languages and implicitly typed languages are classified according to whether the programming language explicitly specifies the type of the variable.

A very confusing issue is that the definition of the type system of programming languages varies from context to context. A typical example is the type system of JavaScript. According to P Thiemann's analysis, JavaScript is an untyped programming language. But according to S Li, JavaScript is a dynamically typed language. Another thing worth discussing is Python. Type hints were introduced in Python in version 3.5. But type hints do not actually do error checking for Python. It is simply a type annotation to help provide better support for static analysis syntax parsers. The real time for error checking is still at runtime.

The above table gives the type system of common MPLs according to L Cardelli's type system definition. It is worth noting that most current MPLs have certain commonalities. They are often explicitly typed, strongly typed, statically typed, and well-behaved. This is inseparable from the application areas of these languages. Type signatures contain constraint information by which the behavior of variables or functions can be indirectly determined. It improves maintainability, which is exactly what programming languages for industrial applications need. Static type checking and formal proofs of type systems improve the reliability of programs and help people write less error-prone code. Also, a static type system is more conducive to performance optimization and memory allocation, and object programs can have better performance.

## Analysis of performance

There are many factors that affect the performance of a programming language, and one cannot simply assume that one language performs better than another. The performance of a programming language is often closely related to the application scenario. Several programming languages may perform very differently in different application scenarios. In order to accurately measure the performance of programming languages, the concept of benchmarking is introduced.

Benchmarking provides a way to systematically test the performance differences of different individuals under the same category and is a scientific way to determine the merits of an individual. In computer science, benchmarking refers to the evaluation of the performance of an object by running a computer program or manipulating some specific behavior. It is done through a series of comparative experiments with controlled variables usually involving several iterative rounds in order to draw reproducible and precise conclusions. In addition, it focuses on a particular procedure and should exclude the influence of unrelated procedures on the benchmark test. This requires that the benchmark test should have a clear idea of how the underlying workings work and avoid errors due to the uncertainty of the system's state.

### Benchmark Test Setup

This test uses Python scripts to perform coarse-grained batch testing uniformly and uses built-in process tools to invoke Linux system commands without relying on third-party libraries, which has high testing efficiency and is, therefore, suitable for large amounts of data. Each test program was executed six times, and the results were averaged to avoid bias. Each benchmark test was executed with larger and smaller scale inputs. For the parameters of the testbed, see Table 1. For the compilation parameters of the programming language, see Table 1.

For the same language with different compilers or compilation methods, compare their differences (e.g. C++, Dart). For the remaining languages, we classify them according to the programming language type system. Because for multiple languages with the same type system and compilation method, there should be overlap in their scope of application in practice, the comparative analysis.

The benchmarking metrics for computer languages come from one of the more popular cross-language benchmarking suites available - the Computer Language Benchmarking Game. For each language, there are four metrics.

+ Compiler. marked after the programming language, if not marked then the official compiler is used.
+ Size. The size of the source code after gzip compression. For the same algorithm, the smaller the amount of code used by a programming language, the more syntactically expressive the language is, in general.
+ CPU. The time required to run the algorithm. Takes the minimum value of multiple runs. Includes startup time.
+ Memory. The peak space consumption to run the algorithm. Takes the maximum of multiple runs.

### Memory allocation test

The idea is derived from Hans-J. Boehm's GC bench algorithm.
The memory allocation capacity and garbage collection capacity are measured by repeatedly allocating and deallocating large amounts of space. The steps are as follows.

As you can see from the table, Java has the best memory allocation and management speed among these programming languages, i.e., it takes very little time. However, Java's memory consumption is relatively the largest among these languages. This is due to the unique memory model of the JVM, which divides the heap area into different generations and uses different garbage collection algorithms for each generation. The advantage of this is obvious, it can greatly increase the efficiency of garbage collection. But at the same time, it takes up more memory than is actually needed for the division. Since Kotlin and Java are both based on the JVM, they have similar performance figures. Kotlin is based on Java8, while Java is based on Java17. From Java9 onwards, the default JVM GC is G1. It has better response time than Parallel, but consumes more memory at the same time. This is in line with the data in the table that Kotlin's time overhead is slightly higher than Java's and memory overhead is slightly lower.

For the two different compilers for C++, Clang and GCC, the memory overhead is almost the same. This is because both of them manage memory manually. However, Clang has a lower time overhead than GCC. This comes from compiler optimizations, and Clang's optimizations are better than GCC's. In addition, the architecture of the compilers is different: Clang-LLVM uses a low-coupling front- and back-end architecture, while GCC uses a front- and back-end coupling architecture. This is due to the fact that GCC is relatively old and limited by its age. We can see that compile-time optimization of the compiler takes a more important role for native compiled languages.

Rust, Go and Swift, which are also strongly typed, compiled MPLs, use three different memory management models. The test results for all three are highly dependent on their respective memory models; Go uses a garbage collection mechanism. Although Go is a compiled language, it has an additional runtime to support garbage collection. Compared to Java, Go uses a more conservative garbage collection strategy and does not aggressively use a space-for-time approach, resulting in decent space consumption and less optimistic runtimes for memory allocation. Rust uses an ownership mechanism, unlike C/C++, which is purely manually managed, and unlike Go's garbage collection. In terms of the underlying implementation, Rust does not actually maintain a runtime to manage space but rather determines the timing of unallocating memory at compile time through a complex arithmetic ownership algorithm. As a result, Rust's memory management is overhead-free. In fact, Rust's compiler backend uses LLVM, the same backend as the Clang-LLVM tested above, and it is also clear from the test data that Rust performs almost identically to Cpp-Clang on floating-point operations. However, due to its ownership mechanism, Rust is better at memory allocation. For Swift, the memory management mechanism is different from all of the above. Swift allocates and deallocates memory by maintaining a runtime reference count. In addition, Swift forces reference counts to be updated atomically instead of Rust's compiler management of thread safety, which results in high CPU overhead. This shows that Swift has a higher runtime overhead compared to Rust, confirming the data that Swift performs poorly compared to Rust.

### Floating-point operation test

The idea comes from the Symplectic Integration algorithm of K. P. Rauch and D. P. Hamilton. This algorithm simulates the evolution of multiple planets and checks the correctness of the program by examining the energy of each evolutionary state. The performance bottleneck is mainly in floating-point operations. The steps are as follows.

Java's time overhead is not very high and does not fall too far behind natively compiled languages. The performance bottleneck of algorithmic programs with short execution time is mainly the JVM startup time and the JIT-optimized warm-up time, and for algorithms that have already done so, Java's execution speed does not lag behind that of natively compiled languages. Java runs in two steps. In the first step, the source code is compiled into bytecode, and in the second step, the JVM interprets and executes the bytecode. In the process of interpreting and executing the bytecode, JVM will receive the runtime information of the collected code, and if it finds that some bytecode is executed more frequently, it will choose to compile this part of the bytecode and compile it into native code, which will be called directly when it is executed again. The so-called performance optimization of Java is mainly in the bytecode, while the source to bytecode stage is only a simple optimization.

There is a significant performance gap between JavaScript and Python. JavaScript runs on Google's V8 engine, which executes JavaScript code much like the JVM executes bytecode, using the same JIT optimization technique. So compared to Python, which does not use JIT optimization, it has a huge performance improvement, even with Java and natively compiled languages with similar floating point performance. In fact, the speed of scripting languages is not a performance bottleneck in most application scenarios. However, in recent years, JavaScript has become the "assembly of the web", and more and more languages are being compiled into JavaScript, such as Kotlin and Dart, so more and more business logic needs to be executed in JavaScript. This makes JavaScript burdened with tedious business-related logic, and optimization of JavaScript is the trend.

### Comprehensive test

The Mandelbrot Set is drawn on a resolution NÃ—N bitmap. the coordinates of the image on the complex axis are [-1.5-i,0.5+i]. For each pixel, a certain number of iterations are performed to determine the gray level of the current pixel. The output in PBF format is performed byte by byte. The correctness is checked by comparing it with the standard output. The performance bottleneck of this test is in floating point operations, memory allocation, and IO.

For the same Dart code, it is compiled in two different ways, using JIT and AOT. As for the time and memory overhead, both have similar performance. This is caused by the compilation mechanism of Dart. In fact, the running mechanism of Dart is different from the traditional way. The traditional AOT is to compile the source code directly into the object code on the target machine, which can be called and run independently by the operating system directly. However, for Dart, whether it is AOT or JIT, only the compilation time is different, and it will eventually run on the virtual machine. In fact, when the compilation time is negligible, the time overhead of AOT and JIT is approximately equal. But this way of running has its unique disadvantage. It will significantly increase the running overhead of AOT mode, but this makes AOT compilation has strong runtime support. Because of this feature, the Dart VM can save the current runtime state, and the next time the VM is started, it can directly load the last state without restarting. This is mainly for a better development experience. The incremental compilation feature allows the runtime results to change almost in real time as the code is changed. Dart is a language for the front-end cross-platform framework Flutter. With such a mechanism, code changes can feed back to the UI in real-time, which is hard to do with other languages.

In industrial applications, the performance bottleneck in most scenarios is compile-related, including not only compile-time but also run-time. The impact of compilation on programming language efficiency is again multifaceted. First, the same programming language with different compilers will often have different object code and thus different runtime efficiency. This is often due to different compile-time optimizations; for example, for C++, there is a significant performance difference in the object code obtained by compiling with Clang-LLVM as opposed to compiling with GCC. Second, the same programming language can run not only in AOT, but also in JIT. For example, for Kotlin, it can be compiled not only as bytecode to run on JVM, but also programmed as JavaScript code to run on the browser, or even Kotlin Native, which can run as native code. In this way, the different compilation methods have a greater impact on efficiency.

The impact of memory management on programming language performance is huge. First, there is the matter of memory allocation on the heap. For languages with VM, memory allocation is an advantage compared to non-VM languages. This is because VM languages provide memory pools that host the memory allocation, whereas non-VM languages do not have such an advantage. Of course, for scenarios where memory on the heap is used frequently, non-VM languages tend to use custom or third-party provided memory pooling frameworks, and the performance gap in the industry is not as pronounced. But managed memory is not always an advantage. When a memory thrash occurs, the VM will frequently GC, which also affects performance. And, memory pools tend to have a larger memory overhead. Second, there is the principle of locality about memory. CPU will put all the adjacent data into cache, and if the memory is accessed sequentially, it greatly improves the hit rate of cache, thus improving the performance.

### Summary

Currently, the analysis of programming language design is still relatively rare compared to the mainstream research direction of PLT. In particular, a systematic approach to the study of programming language design is still lacking. In this paper, we present a relatively comprehensive analysis of programming language design through several aspects affecting programming language applications with the help of several specific MPLs.

In Chapter 2, we discuss the definition of MPL. Based on that, the popular programming languages on IEEE Spectrum with MPL features are selected. Then we discuss the factors that a good programming language design should have. In Chapter 3, we stand in the perspective of programming paradigms, give a method to judge the degree of MPL programming paradigm support, and do an analysis of the degree of support of the selected MPLs. This is followed by an research of the relationship between the strength of programming paradigm support and programming language applications. In Chapter 4, we discuss the type system. The type system of the selected MPLs is discussed based on L Cardelli's type system definition. In Chapter 5, we discuss the performance of programming languages through three benchmark tests in terms of memory allocation and compilation methods. Then, we analyze the data from the benchmark tests from the perspective of programming language implementations and the application scenarios of these languages to explain the reasons for the performance of these MPLs.

Programming paradigms, type systems, and performance are three interconnected parts that cut across multiple aspects of programming languages that need to be considered from design to implementation. Each of these three branches has a significant impact on the design of programming languages, influencing their expressiveness, maintainability, reliability, and performance, the most important metrics of programming languages. In fact we can obtain that the process of theoretical and practical development of programming languages is always dynamic, as is the design and implementation of programming languages. Programming paradigms, type systems, and performance are three interconnected parts that cut across multiple aspects of programming languages that need to be considered from design to implementation. Each of these three branches has a significant impact on the design of programming languages, influencing their expressiveness, maintainability, reliability, and performance, the most important metrics of programming languages. In fact we can obtain that the process of theoretical and practical development of programming languages is always dynamic, as is the design and implementation of programming languages. This paper dynamically discusses programming language design based on MPL, which would be a preferable research idea from a programming language design and analysis perspective.
